# Example environment variables for Summarizarr
# Copy this file to .env and fill in your actual values

# Required: Signal phone number (with country code, e.g., +1234567890)
SIGNAL_PHONE_NUMBER=+1234567890

# Optional: Signal CLI REST API URL (default: signal-cli-rest-api:8080 for Docker)
# For local development, use: localhost:8080
# SIGNAL_URL=localhost:8080

# Optional: How often to generate summaries (default: 12h)
# Examples: 30m, 1h, 6h, 12h, 24h
SUMMARIZATION_INTERVAL=12h


# --- Multi-Provider AI Configuration ---
# AI Provider selection (supports: local, openai, groq, gemini, claude)
AI_PROVIDER=local

# --- Local AI (Ollama) Settings ---
# Optional: Local AI model to use (default: llama3.2:1b)
# Supported: llama3.2:1b, phi3, llama2, mistral, codellama
LOCAL_MODEL=llama3.2:1b

# Optional: Automatically download and start Ollama (default: true)
OLLAMA_AUTO_DOWNLOAD=true

# Optional: Ollama server host and port (default: 127.0.0.1:11434)
OLLAMA_HOST=127.0.0.1:11434

# Optional: How long to keep models in memory (default: 5m)
OLLAMA_KEEP_ALIVE=5m

# --- OpenAI Settings (required when AI_PROVIDER=openai) ---
# OpenAI API key - get from https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-your-api-key-here

# Optional: OpenAI model to use (default: gpt-4o)
# Supported: gpt-4o, gpt-4, gpt-3.5-turbo
# OPENAI_MODEL=gpt-4o

# Optional: OpenAI base URL (default: https://api.openai.com/v1)
# OPENAI_BASE_URL=https://api.openai.com/v1

# --- Groq Settings (required when AI_PROVIDER=groq) ---
# Groq API key - get from https://console.groq.com/keys
# GROQ_API_KEY=gsk-your-api-key-here

# Optional: Groq model to use (default: llama3-8b-8192)
# Supported: llama3-8b-8192, llama3-70b-8192, mixtral-8x7b-32768
# GROQ_MODEL=llama3-8b-8192

# Optional: Groq base URL (default: https://api.groq.com/openai/v1)
# GROQ_BASE_URL=https://api.groq.com/openai/v1

# --- Gemini Settings (required when AI_PROVIDER=gemini) ---
# Gemini API key - get from https://ai.google.dev/
# GEMINI_API_KEY=your-api-key-here

# Optional: Gemini model to use (default: gemini-2.0-flash)
# Supported: gemini-2.0-flash, gemini-1.5-pro, gemini-1.5-flash
# GEMINI_MODEL=gemini-2.0-flash

# Optional: Gemini proxy base URL (default: http://localhost:8000/hf/v1)
# Requires OpenAI-compatible proxy like Gemini Balance
# GEMINI_BASE_URL=http://localhost:8000/hf/v1

# --- Claude Settings (required when AI_PROVIDER=claude) ---
# Claude API key - get from https://console.anthropic.com/
# CLAUDE_API_KEY=sk-ant-your-api-key-here

# Optional: Claude model to use (default: claude-3-sonnet)
# Supported: claude-3-sonnet, claude-3-haiku, claude-3-opus
# CLAUDE_MODEL=claude-3-sonnet

# Optional: Claude proxy base URL (default: http://localhost:8000/openai/v1)
# Requires OpenAI-compatible proxy service
# CLAUDE_BASE_URL=http://localhost:8000/openai/v1

# --- General Settings ---
# Optional: Directory to store AI models (default: ./models)
# For Docker: /app/models
MODELS_PATH=./models

# Optional: Log level (default: INFO)
# Options: DEBUG, INFO, WARN, ERROR
LOG_LEVEL=INFO

# Optional: Database path (default: summarizarr.db)
# For Docker: /app/data/summarizarr.db
# For local development with Docker data: data/summarizarr.db
DATABASE_PATH=data/summarizarr.db
